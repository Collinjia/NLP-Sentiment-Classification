{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74bd92e9",
   "metadata": {},
   "source": [
    "# Sentiment Classification using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be3ee01a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T23:19:10.815596Z",
     "start_time": "2021-06-23T23:19:08.919813Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1465\n",
       "1     297\n",
       "0     158\n",
       "Name: RatingValue, dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This program is a BoW text classifier for sentiment analysis of the reviews.\n",
    "It analyzes the reviews and classify them to three bins: positive, negative\n",
    "and neutral.\n",
    "\n",
    "Created by Ruoxi Jia.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import WordNetLemmatizer, word_tokenize\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import nltk\n",
    "\n",
    "# Import the reviews.csv to dataframe\n",
    "data = pd.read_csv('reviews.csv', sep=',')\n",
    "data['RatingValue'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a93a05",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce6ef1fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T23:19:10.863607Z",
     "start_time": "2021-06-23T23:19:10.817598Z"
    }
   },
   "outputs": [],
   "source": [
    "# Random Drop Undersampling, since we have too many positive reviews\n",
    "data_us = pd.concat([\n",
    "    data.loc[data.RatingValue == 1],\n",
    "    data.loc[data.RatingValue == 0],\n",
    "    data.loc[data.RatingValue == 2].sample(frac=1/4),\n",
    "]).reset_index().drop('index', axis=1)\n",
    "\n",
    "data_us['RatingValue'].value_counts()\n",
    "\n",
    "# Split the dataset into training and valid\n",
    "training, valid = train_test_split(data_us, test_size=0.2, random_state=42)\n",
    "\n",
    "# Export the training and valid dataset into csv file\n",
    "training.to_csv(\"training.csv\")\n",
    "valid.to_csv(\"valid.csv\")\n",
    "\n",
    "# Load the training data\n",
    "train = pd.read_csv(\"training.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff64369",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0d77c18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T23:19:11.086657Z",
     "start_time": "2021-06-23T23:19:10.864608Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\collin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\collin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\collin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# create a tokenizer with lemmatizer\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "\n",
    "    def __call__(self, articles):\n",
    "        # Remove the stop words and lemmatize the rest.\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles) if\n",
    "                t not in nltk.corpus.stopwords.words('english')]\n",
    "\n",
    "\n",
    "# Build a SGD pipeline\n",
    "text_clf = Pipeline([\n",
    "    # Tokenize the text with ngram 1-3 and removing stopwords\n",
    "    ('vect', CountVectorizer(tokenizer=LemmaTokenizer(),\n",
    "                             ngram_range=(1, 3),\n",
    "                             stop_words='english')),\n",
    "    # TF-IDF transform\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    # SGD Classification model\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                          alpha=1e-3, random_state=42,\n",
    "                          max_iter=5, tol=None)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12236091",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T23:20:03.284078Z",
     "start_time": "2021-06-23T23:19:11.087657Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le', 'u'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.62 \n",
      "\n",
      "F1_score: 0.59 \n",
      "\n",
      "Confusion_matrix:\n",
      "          negative  neutral  positive\n",
      "negative         8       19        11\n",
      "neutral          2       36        19\n",
      "positive         0       11        59\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "text_clf.fit(train['Review'], train['RatingValue'])\n",
    "\n",
    "# Import the validation data\n",
    "validation = pd.read_csv(\"valid.csv\")\n",
    "\n",
    "# Implement the model\n",
    "predicted = text_clf.predict(validation['Review'])\n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy = round(np.mean(predicted == validation['RatingValue']), 2)\n",
    "# calculate the F1 score\n",
    "F1_score = round(metrics.f1_score(validation.RatingValue, predicted, average='weighted'), 2)\n",
    "# print out the result\n",
    "print(\"accuracy:\", accuracy, \"\\n\")\n",
    "print(\"F1_score:\", F1_score, \"\\n\")\n",
    "print(\"Confusion_matrix:\")\n",
    "# generate the confusion matrix\n",
    "conf = pd.DataFrame(metrics.confusion_matrix(validation.RatingValue, predicted),\n",
    "                    index=['negative', 'neutral', 'positive'],\n",
    "                    columns=['negative', 'neutral', 'positive'])\n",
    "print(conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdfe818",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
